{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook, shows step-by-step how to perform text classification by fine-tuning a BERT-based model.\n",
    "\n",
    "Here we install the transformers package, from Hugging Face. We use one of its pre-trained BERT models, more specifically a compact model that is trained through model distillation. We will use the package to:\n",
    "\n",
    "Tokenize the text according to the BERT model specification, using its DistilBertTokenizer class\n",
    "Instantiate a pre-trained BERT model, modified for the text classification task, using its DistilBertForSequenceClassification class, that we will then be fine-tuned for our specific dataset.\n",
    "\n",
    "For a comprehensive tutorial about using this package to fine-tune BERT for text classification, please see [here](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing needed python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/consumer_complaint_data_sample_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(52442, 3)"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            Product                                          Complaint  \\\n0  Credit Reporting  i first report on 2019 i asked for a master pr...   \n1  Credit Reporting  please be advised that this is my third writte...   \n2  Credit Reporting  is falsely reporting 9 hard inquiries and want...   \n3  Banking Services  open account on 2018 through the citi bank off...   \n4     Card Services  i was never sent a credit card bill for my pur...   \n5   Debt Collection  hello i am writing to dispute a collection ref...   \n6  Credit Reporting  i was made aware of four accounts today 19 on ...   \n7  Credit Reporting  the credit bureaus are reporting inaccurate ou...   \n8  Credit Reporting  transunion ss dob dear sir or madam i am a vic...   \n9     Card Services  this is my second complaint my first ended up ...   \n\n   Product_Label  \n0              2  \n1              2  \n2              2  \n3              0  \n4              1  \n5              3  \n6              2  \n7              2  \n8              2  \n9              1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product</th>\n      <th>Complaint</th>\n      <th>Product_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Credit Reporting</td>\n      <td>i first report on 2019 i asked for a master pr...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Credit Reporting</td>\n      <td>please be advised that this is my third writte...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Credit Reporting</td>\n      <td>is falsely reporting 9 hard inquiries and want...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Banking Services</td>\n      <td>open account on 2018 through the citi bank off...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Card Services</td>\n      <td>i was never sent a credit card bill for my pur...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Debt Collection</td>\n      <td>hello i am writing to dispute a collection ref...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Credit Reporting</td>\n      <td>i was made aware of four accounts today 19 on ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Credit Reporting</td>\n      <td>the credit bureaus are reporting inaccurate ou...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Credit Reporting</td>\n      <td>transunion ss dob dear sir or madam i am a vic...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Card Services</td>\n      <td>this is my second complaint my first ended up ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  Product\nCredit Reporting    19100\nDebt Collection     11266\nMortgage             6414\nCard Services        5637\nLoans                5343\nBanking Services     4682",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Credit Reporting</th>\n      <td>19100</td>\n    </tr>\n    <tr>\n      <th>Debt Collection</th>\n      <td>11266</td>\n    </tr>\n    <tr>\n      <th>Mortgage</th>\n      <td>6414</td>\n    </tr>\n    <tr>\n      <th>Card Services</th>\n      <td>5637</td>\n    </tr>\n    <tr>\n      <th>Loans</th>\n      <td>5343</td>\n    </tr>\n    <tr>\n      <th>Banking Services</th>\n      <td>4682</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "label_counts = pd.DataFrame(df['Product'].value_counts())\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create an array with the label names in the order they were numerically encoded. We use them later when plotting model performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Banking Services',\n 'Card Services',\n 'Credit Reporting',\n 'Debt Collection',\n 'Loans',\n 'Mortgage']"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "label_values = list(label_counts.index)\n",
    "order = list(pd.DataFrame(df['Product_Label'].value_counts()).index)\n",
    "label_values = [l for _,l in sorted(zip(order, label_values))]\n",
    "\n",
    "label_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create 2 arrays: one with the textual data, which is our feature data, and one with the numerically encoded labels, representing our target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['Complaint'].values\n",
    "labels = df['Product_Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a ‘heavy-weight’´model. This makes the training a very resource-intensive process, specially when we are fine-tuning for all model layers. To mitigate this, we can control the sequence length of our input text, which is given by the number of tokens in our input text, plus 2 special tokens to mark the beginning and ending of a text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5\n5367\n"
    }
   ],
   "source": [
    "text_lengths = [len(texts[i].split()) for i in range(len(texts))]\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8737"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "sum([1 for i in range(len(text_lengths)) if text_lengths[i] >= 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original Text:  i first report on 2019 i asked for a master promissory note proving the loans that they had under my name for there company the cfpb dismissed my complaint because navient sent a promissory note that had nothing to do with the loans they claim they have on file for me \n\ni have attached to this complaint is the promissory note that navient attached with my first complaint i have also attached what navient reporting to the creditors \n\nwhy are the original grantors not the same isnt the information suppose to match \n\ni also asked my private loan carrier for a master promissory note and i was shocked to find out that they have the same promissory note for my private loans that navient have for federal loans for different schools \n\nhow is that possible \n\n has told me on multiple occasions and by multiple employees when i have asked navient who my original grantor is there answer has changed atleast twice and now that i see that there reporting that is the original grantor that s the third misinformation about this loan that they have given not just to me but multiple other companies \n\ni have asked an employee and one of navient managers for contact information to there legal department i was told that navient does not have a legal department \n\nhow do they handle lawsuits without a legal department \n\nall conversations were recorded i have had problems attaching them in the past but if an email is provided i can submit those recordings promptly \n\ni hope someone really looks into this \n\nTokenized Text:  ['i', 'first', 'report', 'on', '2019', 'i', 'asked', 'for', 'a', 'master', 'prom', '##iss', '##ory', 'note', 'proving', 'the', 'loans', 'that', 'they', 'had', 'under', 'my', 'name', 'for', 'there', 'company', 'the', 'cf', '##p', '##b', 'dismissed', 'my', 'complaint', 'because', 'na', '##vie', '##nt', 'sent', 'a', 'prom', '##iss', '##ory', 'note', 'that', 'had', 'nothing', 'to', 'do', 'with', 'the', 'loans', 'they', 'claim', 'they', 'have', 'on', 'file', 'for', 'me', 'i', 'have', 'attached', 'to', 'this', 'complaint', 'is', 'the', 'prom', '##iss', '##ory', 'note', 'that', 'na', '##vie', '##nt', 'attached', 'with', 'my', 'first', 'complaint', 'i', 'have', 'also', 'attached', 'what', 'na', '##vie', '##nt', 'reporting', 'to', 'the', 'creditors', 'why', 'are', 'the', 'original', 'grant', '##ors', 'not', 'the', 'same', 'isn', '##t', 'the', 'information', 'suppose', 'to', 'match', 'i', 'also', 'asked', 'my', 'private', 'loan', 'carrier', 'for', 'a', 'master', 'prom', '##iss', '##ory', 'note', 'and', 'i', 'was', 'shocked', 'to', 'find', 'out', 'that', 'they', 'have', 'the', 'same', 'prom', '##iss', '##ory', 'note', 'for', 'my', 'private', 'loans', 'that', 'na', '##vie', '##nt', 'have', 'for', 'federal', 'loans', 'for', 'different', 'schools', 'how', 'is', 'that', 'possible', 'has', 'told', 'me', 'on', 'multiple', 'occasions', 'and', 'by', 'multiple', 'employees', 'when', 'i', 'have', 'asked', 'na', '##vie', '##nt', 'who', 'my', 'original', 'grant', '##or', 'is', 'there', 'answer', 'has', 'changed', 'at', '##lea', '##st', 'twice', 'and', 'now', 'that', 'i', 'see', 'that', 'there', 'reporting', 'that', 'is', 'the', 'original', 'grant', '##or', 'that', 's', 'the', 'third', 'mis', '##in', '##form', '##ation', 'about', 'this', 'loan', 'that', 'they', 'have', 'given', 'not', 'just', 'to', 'me', 'but', 'multiple', 'other', 'companies', 'i', 'have', 'asked', 'an', 'employee', 'and', 'one', 'of', 'na', '##vie', '##nt', 'managers', 'for', 'contact', 'information', 'to', 'there', 'legal', 'department', 'i', 'was', 'told', 'that', 'na', '##vie', '##nt', 'does', 'not', 'have', 'a', 'legal', 'department', 'how', 'do', 'they', 'handle', 'lawsuits', 'without', 'a', 'legal', 'department', 'all', 'conversations', 'were', 'recorded', 'i', 'have', 'had', 'problems', 'attach', '##ing', 'them', 'in', 'the', 'past', 'but', 'if', 'an', 'email', 'is', 'provided', 'i', 'can', 'submit', 'those', 'recordings', 'promptly', 'i', 'hope', 'someone', 'really', 'looks', 'into', 'this'] \n\nToken IDs:  [1045, 2034, 3189, 2006, 10476, 1045, 2356, 2005, 1037, 3040, 20877, 14643, 10253, 3602, 13946, 1996, 10940, 2008, 2027, 2018, 2104, 2026, 2171, 2005, 2045, 2194, 1996, 12935, 2361, 2497, 7219, 2026, 12087, 2138, 6583, 13469, 3372, 2741, 1037, 20877, 14643, 10253, 3602, 2008, 2018, 2498, 2000, 2079, 2007, 1996, 10940, 2027, 4366, 2027, 2031, 2006, 5371, 2005, 2033, 1045, 2031, 4987, 2000, 2023, 12087, 2003, 1996, 20877, 14643, 10253, 3602, 2008, 6583, 13469, 3372, 4987, 2007, 2026, 2034, 12087, 1045, 2031, 2036, 4987, 2054, 6583, 13469, 3372, 7316, 2000, 1996, 23112, 2339, 2024, 1996, 2434, 3946, 5668, 2025, 1996, 2168, 3475, 2102, 1996, 2592, 6814, 2000, 2674, 1045, 2036, 2356, 2026, 2797, 5414, 6839, 2005, 1037, 3040, 20877, 14643, 10253, 3602, 1998, 1045, 2001, 7135, 2000, 2424, 2041, 2008, 2027, 2031, 1996, 2168, 20877, 14643, 10253, 3602, 2005, 2026, 2797, 10940, 2008, 6583, 13469, 3372, 2031, 2005, 2976, 10940, 2005, 2367, 2816, 2129, 2003, 2008, 2825, 2038, 2409, 2033, 2006, 3674, 6642, 1998, 2011, 3674, 5126, 2043, 1045, 2031, 2356, 6583, 13469, 3372, 2040, 2026, 2434, 3946, 2953, 2003, 2045, 3437, 2038, 2904, 2012, 19738, 3367, 3807, 1998, 2085, 2008, 1045, 2156, 2008, 2045, 7316, 2008, 2003, 1996, 2434, 3946, 2953, 2008, 1055, 1996, 2353, 28616, 2378, 14192, 3370, 2055, 2023, 5414, 2008, 2027, 2031, 2445, 2025, 2074, 2000, 2033, 2021, 3674, 2060, 3316, 1045, 2031, 2356, 2019, 7904, 1998, 2028, 1997, 6583, 13469, 3372, 10489, 2005, 3967, 2592, 2000, 2045, 3423, 2533, 1045, 2001, 2409, 2008, 6583, 13469, 3372, 2515, 2025, 2031, 1037, 3423, 2533, 2129, 2079, 2027, 5047, 20543, 2302, 1037, 3423, 2533, 2035, 11450, 2020, 2680, 1045, 2031, 2018, 3471, 22476, 2075, 2068, 1999, 1996, 2627, 2021, 2065, 2019, 10373, 2003, 3024, 1045, 2064, 12040, 2216, 5633, 13364, 1045, 3246, 2619, 2428, 3504, 2046, 2023]\n"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "print('Original Text: ', texts[0], '\\n')\n",
    "print('Tokenized Text: ', tokenizer.tokenize(texts[0]), '\\n')\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenize and encode the entire dataset. In this process, we perform the following:\n",
    "\n",
    ". tokenize the text as shown above\n",
    ". encode it to the corresponding numeric values for each token.\n",
    ". truncate it to the maximum sequence length of 300.\n",
    ". pad the tokens positions greater than 300.\n",
    ". include the special token IDs to mark the beginning and end of each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[101,\n 1045,\n 2034,\n 3189,\n 2006,\n 10476,\n 1045,\n 2356,\n 2005,\n 1037,\n 3040,\n 20877,\n 14643,\n 10253,\n 3602,\n 13946,\n 1996,\n 10940,\n 2008,\n 2027,\n 2018,\n 2104,\n 2026,\n 2171,\n 2005,\n 2045,\n 2194,\n 1996,\n 12935,\n 2361,\n 2497,\n 7219,\n 2026,\n 12087,\n 2138,\n 6583,\n 13469,\n 3372,\n 2741,\n 1037,\n 20877,\n 14643,\n 10253,\n 3602,\n 2008,\n 2018,\n 2498,\n 2000,\n 2079,\n 2007,\n 1996,\n 10940,\n 2027,\n 4366,\n 2027,\n 2031,\n 2006,\n 5371,\n 2005,\n 2033,\n 1045,\n 2031,\n 4987,\n 2000,\n 2023,\n 12087,\n 2003,\n 1996,\n 20877,\n 14643,\n 10253,\n 3602,\n 2008,\n 6583,\n 13469,\n 3372,\n 4987,\n 2007,\n 2026,\n 2034,\n 12087,\n 1045,\n 2031,\n 2036,\n 4987,\n 2054,\n 6583,\n 13469,\n 3372,\n 7316,\n 2000,\n 1996,\n 23112,\n 2339,\n 2024,\n 1996,\n 2434,\n 3946,\n 5668,\n 2025,\n 1996,\n 2168,\n 3475,\n 2102,\n 1996,\n 2592,\n 6814,\n 2000,\n 2674,\n 1045,\n 2036,\n 2356,\n 2026,\n 2797,\n 5414,\n 6839,\n 2005,\n 1037,\n 3040,\n 20877,\n 14643,\n 10253,\n 3602,\n 1998,\n 1045,\n 2001,\n 7135,\n 2000,\n 2424,\n 2041,\n 2008,\n 2027,\n 2031,\n 1996,\n 2168,\n 20877,\n 14643,\n 10253,\n 3602,\n 2005,\n 2026,\n 2797,\n 10940,\n 2008,\n 6583,\n 13469,\n 3372,\n 2031,\n 2005,\n 2976,\n 10940,\n 2005,\n 2367,\n 2816,\n 2129,\n 2003,\n 2008,\n 2825,\n 2038,\n 2409,\n 2033,\n 2006,\n 3674,\n 6642,\n 1998,\n 2011,\n 3674,\n 5126,\n 2043,\n 1045,\n 2031,\n 2356,\n 6583,\n 13469,\n 3372,\n 2040,\n 2026,\n 2434,\n 3946,\n 2953,\n 2003,\n 2045,\n 3437,\n 2038,\n 2904,\n 2012,\n 19738,\n 3367,\n 3807,\n 1998,\n 2085,\n 2008,\n 1045,\n 2156,\n 2008,\n 2045,\n 7316,\n 2008,\n 2003,\n 1996,\n 2434,\n 3946,\n 2953,\n 2008,\n 1055,\n 1996,\n 2353,\n 28616,\n 2378,\n 14192,\n 3370,\n 2055,\n 2023,\n 5414,\n 2008,\n 2027,\n 2031,\n 2445,\n 2025,\n 2074,\n 2000,\n 2033,\n 2021,\n 3674,\n 2060,\n 3316,\n 1045,\n 2031,\n 2356,\n 2019,\n 7904,\n 1998,\n 2028,\n 1997,\n 6583,\n 13469,\n 3372,\n 10489,\n 2005,\n 3967,\n 2592,\n 2000,\n 2045,\n 3423,\n 2533,\n 1045,\n 2001,\n 2409,\n 2008,\n 6583,\n 13469,\n 3372,\n 2515,\n 2025,\n 2031,\n 1037,\n 3423,\n 2533,\n 2129,\n 2079,\n 2027,\n 5047,\n 20543,\n 2302,\n 1037,\n 3423,\n 2533,\n 2035,\n 11450,\n 2020,\n 2680,\n 1045,\n 2031,\n 2018,\n 3471,\n 22476,\n 2075,\n 2068,\n 1999,\n 1996,\n 2627,\n 2021,\n 2065,\n 2019,\n 10373,\n 2003,\n 3024,\n 1045,\n 2064,\n 12040,\n 2216,\n 5633,\n 13364,\n 1045,\n 3246,\n 2619,\n 2428,\n 3504,\n 2046,\n 102]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "text_ids = [tokenizer.encode(text, max_length=300, pad_to_max_length=True) for text in texts]\n",
    "\n",
    "text_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "300\n300\n"
    }
   ],
   "source": [
    "text_ids_lengths = [len(text_ids[i]) for i in range(len(text_ids))]\n",
    "print(min(text_ids_lengths))\n",
    "print(max(text_ids_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune our model, we need two inputs: one array of token IDs (created above) and one array of a corresponding binary mask, called attention mask in the BERT model specification. Each attention mask has the same length of the corresponding input sequence and has a 0 if the corresponding token is a pad token, or a 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "att_masks = []\n",
    "for ids in text_ids:\n",
    "    masks = [int(id > 0) for id in ids]\n",
    "    att_masks.append(masks)\n",
    "    \n",
    "att_masks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the input and output arrays created before into train, validation, and test sets. We use 80% of the data for training, 10% for training validation, and 10% for final testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_val_x, train_y, test_val_y = train_test_split(text_ids, labels, random_state=111, test_size=0.2)\n",
    "train_m, test_val_m = train_test_split(att_masks, random_state=111, test_size=0.2)\n",
    "\n",
    "test_x, val_x, test_y, val_y = train_test_split(test_val_x, test_val_y, random_state=111, test_size=0.5)\n",
    "test_m, val_m = train_test_split(test_val_m, random_state=111, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with the PyTorch artifacts in the transformers library, therefore we need our model input and output data as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([41953, 300])\ntorch.Size([5244, 300])\ntorch.Size([5245, 300])\ntorch.Size([41953])\ntorch.Size([5244])\ntorch.Size([5245])\ntorch.Size([41953, 300])\ntorch.Size([5244, 300])\ntorch.Size([5245, 300])\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_x = torch.tensor(train_x)\n",
    "test_x = torch.tensor(test_x)\n",
    "val_x = torch.tensor(val_x)\n",
    "train_y = torch.tensor(train_y)\n",
    "test_y = torch.tensor(test_y)\n",
    "val_y = torch.tensor(val_y)\n",
    "train_m = torch.tensor(train_m)\n",
    "test_m = torch.tensor(test_m)\n",
    "val_m = torch.tensor(val_m)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(val_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)\n",
    "print(val_y.shape)\n",
    "print(train_m.shape)\n",
    "print(test_m.shape)\n",
    "print(val_m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed data into the model for training, we use Pytorch’s Dataset, DataLoader, and Sampler. For feeding training data, which drives model weights updates, we use the RandomSampler. For feeding the validation data we can use the SequentialSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_x, train_m, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_x, val_m, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate our model class. We use a compact version, that is trained through model distillation from a base BERT model and modified to include a classification layer at the output. This compact version has 6 transformer layers instead of 12 as in the original BERT model. Please see here for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig\n",
    "\n",
    "num_labels = len(set(labels))\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels,\n",
    "                                                            output_attentions=False, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a very large model. Unless you are freezing model weights in all layers but the classification layer, it is recommended to train it on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print the model architecture and all model learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of trainable parameters: 66958086 \n DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=6, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of trainable parameters:', count_parameters(model), '\\n', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['distilbert.embeddings.word_embeddings.weight',\n 'distilbert.embeddings.position_embeddings.weight',\n 'distilbert.embeddings.LayerNorm.weight',\n 'distilbert.embeddings.LayerNorm.bias',\n 'distilbert.transformer.layer.0.attention.q_lin.weight',\n 'distilbert.transformer.layer.0.attention.q_lin.bias',\n 'distilbert.transformer.layer.0.attention.k_lin.weight',\n 'distilbert.transformer.layer.0.attention.k_lin.bias',\n 'distilbert.transformer.layer.0.attention.v_lin.weight',\n 'distilbert.transformer.layer.0.attention.v_lin.bias',\n 'distilbert.transformer.layer.0.attention.out_lin.weight',\n 'distilbert.transformer.layer.0.attention.out_lin.bias',\n 'distilbert.transformer.layer.0.sa_layer_norm.weight',\n 'distilbert.transformer.layer.0.sa_layer_norm.bias',\n 'distilbert.transformer.layer.0.ffn.lin1.weight',\n 'distilbert.transformer.layer.0.ffn.lin1.bias',\n 'distilbert.transformer.layer.0.ffn.lin2.weight',\n 'distilbert.transformer.layer.0.ffn.lin2.bias',\n 'distilbert.transformer.layer.0.output_layer_norm.weight',\n 'distilbert.transformer.layer.0.output_layer_norm.bias',\n 'distilbert.transformer.layer.1.attention.q_lin.weight',\n 'distilbert.transformer.layer.1.attention.q_lin.bias',\n 'distilbert.transformer.layer.1.attention.k_lin.weight',\n 'distilbert.transformer.layer.1.attention.k_lin.bias',\n 'distilbert.transformer.layer.1.attention.v_lin.weight',\n 'distilbert.transformer.layer.1.attention.v_lin.bias',\n 'distilbert.transformer.layer.1.attention.out_lin.weight',\n 'distilbert.transformer.layer.1.attention.out_lin.bias',\n 'distilbert.transformer.layer.1.sa_layer_norm.weight',\n 'distilbert.transformer.layer.1.sa_layer_norm.bias',\n 'distilbert.transformer.layer.1.ffn.lin1.weight',\n 'distilbert.transformer.layer.1.ffn.lin1.bias',\n 'distilbert.transformer.layer.1.ffn.lin2.weight',\n 'distilbert.transformer.layer.1.ffn.lin2.bias',\n 'distilbert.transformer.layer.1.output_layer_norm.weight',\n 'distilbert.transformer.layer.1.output_layer_norm.bias',\n 'distilbert.transformer.layer.2.attention.q_lin.weight',\n 'distilbert.transformer.layer.2.attention.q_lin.bias',\n 'distilbert.transformer.layer.2.attention.k_lin.weight',\n 'distilbert.transformer.layer.2.attention.k_lin.bias',\n 'distilbert.transformer.layer.2.attention.v_lin.weight',\n 'distilbert.transformer.layer.2.attention.v_lin.bias',\n 'distilbert.transformer.layer.2.attention.out_lin.weight',\n 'distilbert.transformer.layer.2.attention.out_lin.bias',\n 'distilbert.transformer.layer.2.sa_layer_norm.weight',\n 'distilbert.transformer.layer.2.sa_layer_norm.bias',\n 'distilbert.transformer.layer.2.ffn.lin1.weight',\n 'distilbert.transformer.layer.2.ffn.lin1.bias',\n 'distilbert.transformer.layer.2.ffn.lin2.weight',\n 'distilbert.transformer.layer.2.ffn.lin2.bias',\n 'distilbert.transformer.layer.2.output_layer_norm.weight',\n 'distilbert.transformer.layer.2.output_layer_norm.bias',\n 'distilbert.transformer.layer.3.attention.q_lin.weight',\n 'distilbert.transformer.layer.3.attention.q_lin.bias',\n 'distilbert.transformer.layer.3.attention.k_lin.weight',\n 'distilbert.transformer.layer.3.attention.k_lin.bias',\n 'distilbert.transformer.layer.3.attention.v_lin.weight',\n 'distilbert.transformer.layer.3.attention.v_lin.bias',\n 'distilbert.transformer.layer.3.attention.out_lin.weight',\n 'distilbert.transformer.layer.3.attention.out_lin.bias',\n 'distilbert.transformer.layer.3.sa_layer_norm.weight',\n 'distilbert.transformer.layer.3.sa_layer_norm.bias',\n 'distilbert.transformer.layer.3.ffn.lin1.weight',\n 'distilbert.transformer.layer.3.ffn.lin1.bias',\n 'distilbert.transformer.layer.3.ffn.lin2.weight',\n 'distilbert.transformer.layer.3.ffn.lin2.bias',\n 'distilbert.transformer.layer.3.output_layer_norm.weight',\n 'distilbert.transformer.layer.3.output_layer_norm.bias',\n 'distilbert.transformer.layer.4.attention.q_lin.weight',\n 'distilbert.transformer.layer.4.attention.q_lin.bias',\n 'distilbert.transformer.layer.4.attention.k_lin.weight',\n 'distilbert.transformer.layer.4.attention.k_lin.bias',\n 'distilbert.transformer.layer.4.attention.v_lin.weight',\n 'distilbert.transformer.layer.4.attention.v_lin.bias',\n 'distilbert.transformer.layer.4.attention.out_lin.weight',\n 'distilbert.transformer.layer.4.attention.out_lin.bias',\n 'distilbert.transformer.layer.4.sa_layer_norm.weight',\n 'distilbert.transformer.layer.4.sa_layer_norm.bias',\n 'distilbert.transformer.layer.4.ffn.lin1.weight',\n 'distilbert.transformer.layer.4.ffn.lin1.bias',\n 'distilbert.transformer.layer.4.ffn.lin2.weight',\n 'distilbert.transformer.layer.4.ffn.lin2.bias',\n 'distilbert.transformer.layer.4.output_layer_norm.weight',\n 'distilbert.transformer.layer.4.output_layer_norm.bias',\n 'distilbert.transformer.layer.5.attention.q_lin.weight',\n 'distilbert.transformer.layer.5.attention.q_lin.bias',\n 'distilbert.transformer.layer.5.attention.k_lin.weight',\n 'distilbert.transformer.layer.5.attention.k_lin.bias',\n 'distilbert.transformer.layer.5.attention.v_lin.weight',\n 'distilbert.transformer.layer.5.attention.v_lin.bias',\n 'distilbert.transformer.layer.5.attention.out_lin.weight',\n 'distilbert.transformer.layer.5.attention.out_lin.bias',\n 'distilbert.transformer.layer.5.sa_layer_norm.weight',\n 'distilbert.transformer.layer.5.sa_layer_norm.bias',\n 'distilbert.transformer.layer.5.ffn.lin1.weight',\n 'distilbert.transformer.layer.5.ffn.lin1.bias',\n 'distilbert.transformer.layer.5.ffn.lin2.weight',\n 'distilbert.transformer.layer.5.ffn.lin2.bias',\n 'distilbert.transformer.layer.5.output_layer_norm.weight',\n 'distilbert.transformer.layer.5.output_layer_norm.bias',\n 'pre_classifier.weight',\n 'pre_classifier.bias',\n 'classifier.weight',\n 'classifier.bias']"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "[n for n, p in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following 5 cells we define our PyTorch optimizer and corresponding parameters, learning rate scheduler, and the training loop for the fine-tuning procedure. We train for 1 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_epochs = 1\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed_val = 111\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6bc8a3292970>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mmb_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmb_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmb_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmb_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[0;32m    609\u001b[0m         \"\"\"\n\u001b[0;32m    610\u001b[0m         distilbert_output = self.distilbert(\n\u001b[1;32m--> 611\u001b[1;33m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m         )\n\u001b[0;32m    613\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistilbert_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# (bs, seq_len, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds)\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, seq_length, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m         \u001b[0mtfmr_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfmr_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtfmr_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, attn_mask, head_mask)\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m             \u001b[0mlayer_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, attn_mask, head_mask)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;31m# Feed Forward Network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, seq_length, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mffn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, seq_length, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mffn_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         return F.layer_norm(\n\u001b[1;32m--> 153\u001b[1;33m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   1694\u001b[0m     \"\"\"\n\u001b[0;32m   1695\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[1;32m-> 1696\u001b[1;33m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[0;32m   1697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "num_mb_train = len(train_dataloader)\n",
    "num_mb_val = len(val_dataloader)\n",
    "\n",
    "if num_mb_val == 0:\n",
    "    num_mb_val = 1\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for k, (mb_x, mb_m, mb_y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        \n",
    "        mb_x = mb_x.to(device)\n",
    "        mb_m = mb_m.to(device)\n",
    "        mb_y = mb_y.to(device)\n",
    "        \n",
    "        outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        #loss = model_loss(outputs[1], mb_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.data / num_mb_train\n",
    "    \n",
    "    print (\"\\nTrain loss after itaration %i: %f\" % (n+1, train_loss))\n",
    "    train_losses.append(train_loss.cpu())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for k, (mb_x, mb_m, mb_y) in enumerate(val_dataloader):\n",
    "            mb_x = mb_x.to(device)\n",
    "            mb_m = mb_m.to(device)\n",
    "            mb_y = mb_y.to(device)\n",
    "        \n",
    "            outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            #loss = model_loss(outputs[1], mb_y)\n",
    "            \n",
    "            val_loss += loss.data / num_mb_val\n",
    "            \n",
    "        print (\"Validation loss after itaration %i: %f\" % (n+1, val_loss))\n",
    "        val_losses.append(val_loss.cpu())\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Time: {epoch_mins}m {epoch_secs}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can save the model and necessary configuration parameters, to recreate it later and use it to score the test data. Here we also save the losses computed from both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "out_dir = './model'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "with open(out_dir + '/train_losses.pkl', 'wb') as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "    \n",
    "with open(out_dir + '/val_losses.pkl', 'wb') as f:\n",
    "    pickle.dump(val_losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './model'\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(out_dir)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "with open(out_dir + '/train_losses.pkl', 'rb') as f:\n",
    "    train_losses = pickle.load(f)\n",
    "    \n",
    "with open(out_dir + '/val_losses.pkl', 'rb') as f:\n",
    "    val_losses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After instantiating a trained model, we can then score the test data and compute its accuracy. We then print the classification report and plot a confusion matrix.\n",
    "\n",
    "The trained model gives us good results on the test data, being able to correctly classify 80% or more on each of the 6 distinct categories."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "mailbot-demo-01",
   "display_name": "Python (mailbot-demo-01)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}